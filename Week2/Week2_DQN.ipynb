{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "ca209422",
      "metadata": {
        "id": "ca209422"
      },
      "outputs": [],
      "source": [
        "# Imports\n",
        "import torch\n",
        "import numpy as np\n",
        "import gymnasium as gym\n",
        "from collections import deque\n",
        "import pygame\n",
        "import random\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class DQN(nn.Module):\n",
        "    def __init__(self, input_dim, output_dim):\n",
        "        super(DQN, self).__init__()\n",
        "        self.l1 = nn.Linear(input_dim, 128)\n",
        "        self.l2 = nn.Linear(128, 128)\n",
        "        self.out = nn.Linear(128, output_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.leaky_relu(self.l1(x), negative_slope=0.01)\n",
        "        x = F.leaky_relu(self.l2(x), negative_slope=0.01)\n",
        "        return self.out(x)"
      ],
      "metadata": {
        "id": "_lVGPPMw-bZQ"
      },
      "id": "_lVGPPMw-bZQ",
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "37adca73",
      "metadata": {
        "id": "37adca73"
      },
      "outputs": [],
      "source": [
        "# While training neural networks, we split the data into batches.\n",
        "# To improve the training, we need to remove the \"correlation\" between game states\n",
        "# The buffer starts storing states and once it reaches maximum capacity, it replaces\n",
        "# states at random which reduces the correlation.\n",
        "class ExperienceBuffer:\n",
        "    def __init__(self, capacity):\n",
        "        self.buffer = deque(maxlen=capacity)\n",
        "\n",
        "    def push(self, state, action, reward, next_state, done):\n",
        "        self.buffer.append((state, action, reward, next_state, done))\n",
        "\n",
        "    def sample(self, batch_size):\n",
        "        batch = random.sample(self.buffer, batch_size)\n",
        "        states, actions, rewards, next_states, dones = zip(*batch)\n",
        "        return states, actions, rewards, next_states, dones\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.buffer)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def train_dqn_cartpole():\n",
        "    env = gym.make(\"CartPole-v1\")\n",
        "    state_dim = env.observation_space.shape[0]\n",
        "    action_dim = env.action_space.n\n",
        "\n",
        "    policy_net = DQN(state_dim, action_dim)\n",
        "    target_net = DQN(state_dim, action_dim)\n",
        "    target_net.load_state_dict(policy_net.state_dict())\n",
        "    target_net.eval()\n",
        "\n",
        "    optimizer = optim.Adam(policy_net.parameters(), lr=1e-3)\n",
        "    buffer = ExperienceBuffer(10000)\n",
        "\n",
        "    batch_size = 64\n",
        "    gamma = 0.99\n",
        "    episodes = 500\n",
        "    target_update = 10\n",
        "    epsilon = 1.0\n",
        "    decay = 0.995\n",
        "    epsilon_min = 0.01\n",
        "\n",
        "    for episode in range(episodes):\n",
        "        obs, _ = env.reset()\n",
        "        total_reward = 0\n",
        "        done = False\n",
        "\n",
        "        while not done:\n",
        "            state = torch.tensor(obs, dtype=torch.float32).unsqueeze(0)\n",
        "\n",
        "            if random.random() < epsilon:\n",
        "                action = env.action_space.sample()\n",
        "            else:\n",
        "                with torch.no_grad():\n",
        "                    q_values = policy_net(state)\n",
        "                action = torch.argmax(q_values, dim=1).item()\n",
        "\n",
        "            next_obs, reward, done, _, _ = env.step(action)\n",
        "            buffer.push(obs, reward, action, next_obs, done)\n",
        "\n",
        "            obs = next_obs\n",
        "            total_reward += reward\n",
        "\n",
        "            if len(buffer) > batch_size:\n",
        "                states, rewards, actions, next_states, dones = buffer.sample(batch_size)\n",
        "                states = torch.tensor(states, dtype=torch.float32)  # Convert to tensor\n",
        "                actions = torch.tensor(actions, dtype=torch.long)\n",
        "                rewards = torch.tensor(rewards, dtype=torch.float32)\n",
        "                next_states = torch.tensor(next_states, dtype=torch.float32)\n",
        "                dones = torch.tensor(dones, dtype=torch.float32)\n",
        "\n",
        "                q_values = policy_net(states).gather(1, actions.unsqueeze(1)).squeeze()\n",
        "\n",
        "                with torch.no_grad():\n",
        "                    q_next = target_net(next_states).max(1)[0]\n",
        "                    target = rewards + gamma * q_next * (1 - dones)\n",
        "\n",
        "                loss = F.mse_loss(q_values, target)\n",
        "                optimizer.zero_grad()\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "\n",
        "        if episode % target_update == 0:\n",
        "            target_net.load_state_dict(policy_net.state_dict())\n",
        "\n",
        "        epsilon = max(epsilon_min, epsilon * decay)\n",
        "\n",
        "        if (episode + 1) % 10 == 0:\n",
        "            print(f\"Episode {episode + 1}: Total Reward = {total_reward}, Epsilon = {epsilon:.3f}\")\n",
        "\n",
        "    env.close()\n",
        "    return policy_net"
      ],
      "metadata": {
        "id": "obsSuj-YZklP"
      },
      "id": "obsSuj-YZklP",
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "6493553c",
      "metadata": {
        "id": "6493553c"
      },
      "outputs": [],
      "source": [
        "def evaluate_cartpole_model(model, episodes=100, render=True):\n",
        "    env = gym.make(\"CartPole-v1\", render_mode=\"human\" if render else None)\n",
        "    obs_dim = env.observation_space.shape[0]\n",
        "    n_actions = env.action_space.n\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    rewards = []\n",
        "\n",
        "    for episode in range(episodes):\n",
        "        obs, _ = env.reset()\n",
        "        total_reward = 0\n",
        "        done = False\n",
        "\n",
        "        while not done:\n",
        "            state = torch.tensor(obs, dtype=torch.float32).unsqueeze(0)\n",
        "            with torch.no_grad():\n",
        "                q_values = model(state)\n",
        "                action = torch.argmax(q_values, dim=1).item()\n",
        "\n",
        "            obs, reward, done, _, _ = env.step(action)\n",
        "            total_reward += reward\n",
        "\n",
        "            if render:\n",
        "                env.render()\n",
        "\n",
        "        rewards.append(total_reward)\n",
        "        print(f\"Episode {episode + 1}: Reward = {total_reward}\")\n",
        "\n",
        "    env.close()\n",
        "    avg_reward = sum(rewards) / episodes\n",
        "    print(f\"Average reward over {episodes} episodes: {avg_reward}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "8643afe7",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8643afe7",
        "outputId": "ddfad37b-91ca-4e9b-825c-12e607c1eb59"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-5-1956965271.py:45: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /pytorch/torch/csrc/utils/tensor_new.cpp:254.)\n",
            "  states = torch.tensor(states, dtype=torch.float32)  # Convert to tensor\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode 10: Total Reward = 25.0, Epsilon = 0.951\n",
            "Episode 20: Total Reward = 28.0, Epsilon = 0.905\n",
            "Episode 30: Total Reward = 11.0, Epsilon = 0.860\n",
            "Episode 40: Total Reward = 47.0, Epsilon = 0.818\n",
            "Episode 50: Total Reward = 29.0, Epsilon = 0.778\n",
            "Episode 60: Total Reward = 20.0, Epsilon = 0.740\n",
            "Episode 70: Total Reward = 51.0, Epsilon = 0.704\n",
            "Episode 80: Total Reward = 17.0, Epsilon = 0.670\n",
            "Episode 90: Total Reward = 17.0, Epsilon = 0.637\n",
            "Episode 100: Total Reward = 119.0, Epsilon = 0.606\n",
            "Episode 110: Total Reward = 19.0, Epsilon = 0.576\n",
            "Episode 120: Total Reward = 14.0, Epsilon = 0.548\n",
            "Episode 130: Total Reward = 16.0, Epsilon = 0.521\n",
            "Episode 140: Total Reward = 33.0, Epsilon = 0.496\n",
            "Episode 150: Total Reward = 114.0, Epsilon = 0.471\n",
            "Episode 160: Total Reward = 118.0, Epsilon = 0.448\n",
            "Episode 170: Total Reward = 119.0, Epsilon = 0.427\n",
            "Episode 180: Total Reward = 112.0, Epsilon = 0.406\n",
            "Episode 190: Total Reward = 45.0, Epsilon = 0.386\n",
            "Episode 200: Total Reward = 49.0, Epsilon = 0.367\n",
            "Episode 210: Total Reward = 99.0, Epsilon = 0.349\n",
            "Episode 220: Total Reward = 81.0, Epsilon = 0.332\n",
            "Episode 230: Total Reward = 99.0, Epsilon = 0.316\n",
            "Episode 240: Total Reward = 90.0, Epsilon = 0.300\n",
            "Episode 250: Total Reward = 15.0, Epsilon = 0.286\n",
            "Episode 260: Total Reward = 101.0, Epsilon = 0.272\n",
            "Episode 270: Total Reward = 116.0, Epsilon = 0.258\n",
            "Episode 280: Total Reward = 115.0, Epsilon = 0.246\n",
            "Episode 290: Total Reward = 252.0, Epsilon = 0.234\n",
            "Episode 300: Total Reward = 540.0, Epsilon = 0.222\n",
            "Episode 310: Total Reward = 291.0, Epsilon = 0.211\n",
            "Episode 320: Total Reward = 294.0, Epsilon = 0.201\n",
            "Episode 330: Total Reward = 70.0, Epsilon = 0.191\n",
            "Episode 340: Total Reward = 77.0, Epsilon = 0.182\n",
            "Episode 350: Total Reward = 134.0, Epsilon = 0.173\n",
            "Episode 360: Total Reward = 287.0, Epsilon = 0.165\n",
            "Episode 370: Total Reward = 59.0, Epsilon = 0.157\n",
            "Episode 380: Total Reward = 328.0, Epsilon = 0.149\n",
            "Episode 390: Total Reward = 194.0, Epsilon = 0.142\n",
            "Episode 400: Total Reward = 108.0, Epsilon = 0.135\n",
            "Episode 410: Total Reward = 115.0, Epsilon = 0.128\n",
            "Episode 420: Total Reward = 111.0, Epsilon = 0.122\n",
            "Episode 430: Total Reward = 112.0, Epsilon = 0.116\n",
            "Episode 440: Total Reward = 130.0, Epsilon = 0.110\n",
            "Episode 450: Total Reward = 175.0, Epsilon = 0.105\n",
            "Episode 460: Total Reward = 140.0, Epsilon = 0.100\n",
            "Episode 470: Total Reward = 130.0, Epsilon = 0.095\n",
            "Episode 480: Total Reward = 127.0, Epsilon = 0.090\n",
            "Episode 490: Total Reward = 167.0, Epsilon = 0.086\n",
            "Episode 500: Total Reward = 169.0, Epsilon = 0.082\n",
            "Episode 1: Reward = 165.0\n",
            "Episode 2: Reward = 158.0\n",
            "Episode 3: Reward = 153.0\n",
            "Episode 4: Reward = 159.0\n",
            "Episode 5: Reward = 163.0\n",
            "Episode 6: Reward = 156.0\n",
            "Episode 7: Reward = 164.0\n",
            "Episode 8: Reward = 162.0\n",
            "Episode 9: Reward = 162.0\n",
            "Episode 10: Reward = 164.0\n",
            "Average reward over 10 episodes: 160.6\n"
          ]
        }
      ],
      "source": [
        "\n",
        "trained_model = train_dqn_cartpole()\n",
        "evaluate_cartpole_model(trained_model, episodes=10, render=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "id": "b3c4213b",
      "metadata": {
        "id": "b3c4213b"
      },
      "outputs": [],
      "source": [
        "class SnakeGame(gym.Env):\n",
        "    metadata = {\"render_modes\": [\"human\"], \"render_fps\": 10}\n",
        "\n",
        "    def __init__(self, size=10, render_mode=None):\n",
        "        super().__init__()\n",
        "        self.size = size\n",
        "        self.cell_size = 30\n",
        "        self.screen_size = self.size * self.cell_size\n",
        "        self.render_mode = render_mode\n",
        "\n",
        "        self.action_space = gym.spaces.Discrete(4)  # 0: right, 1: up, 2: left, 3: down\n",
        "        self.observation_space = gym.spaces.Box(0, 2, shape=(self.size, self.size), dtype=np.uint8)\n",
        "\n",
        "        self.screen = None\n",
        "        self.clock = None\n",
        "\n",
        "        self.snake = deque()\n",
        "        self.food = None\n",
        "        self.direction = [1, 0]\n",
        "\n",
        "        if self.render_mode == \"human\":\n",
        "            pygame.init()\n",
        "            self.screen = pygame.display.set_mode((self.screen_size, self.screen_size))\n",
        "            self.clock = pygame.time.Clock()\n",
        "\n",
        "    def reset(self, seed=None, options=None):\n",
        "        super().reset(seed=seed)\n",
        "        self.snake.clear()\n",
        "        mid = self.size // 2\n",
        "        self.snake.appendleft([mid, mid])\n",
        "        self.direction = [1, 0]\n",
        "        self._place_food()\n",
        "        obs = self._get_obs()\n",
        "\n",
        "        if self.render_mode == \"human\":\n",
        "            self._render_init()\n",
        "\n",
        "        return obs, {}\n",
        "\n",
        "    def step(self, action):\n",
        "        # TODO: Change reward schema to avoid the following\n",
        "        # 1) 180 degree turns\n",
        "        # 2) Wall collisions\n",
        "        # 3) Being slow at collecting food\n",
        "\n",
        "        if action == 0 and self.direction != [-1, 0]: self.direction = [1, 0]\n",
        "        elif action == 1 and self.direction != [0, 1]: self.direction = [0, -1]\n",
        "        elif action == 2 and self.direction != [1, 0]: self.direction = [-1, 0]\n",
        "        elif action == 3 and self.direction != [0, -1]: self.direction = [0, 1]\n",
        "\n",
        "        head = self.snake[0]\n",
        "        new_head = [head[0] + self.direction[0], head[1] + self.direction[1]]\n",
        "\n",
        "        done = False\n",
        "        reward = 0\n",
        "\n",
        "        if not (0 <= new_head[0] < self.size and 0 <= new_head[1] < self.size):\n",
        "            done = True\n",
        "            reward-=1\n",
        "        else:\n",
        "            body_to_check = list(self.snake)[:-1] if new_head != self.food else list(self.snake)\n",
        "            if new_head in body_to_check:\n",
        "                done=True\n",
        "                reward -=1\n",
        "\n",
        "        if not done:\n",
        "            self.snake.appendleft(new_head)\n",
        "            if new_head == self.food:\n",
        "                self._place_food()\n",
        "                reward+=10\n",
        "            else:\n",
        "                self.snake.pop()\n",
        "                reward-=0.1\n",
        "\n",
        "        obs = self._get_obs()\n",
        "\n",
        "        if self.render_mode == \"human\":\n",
        "            self.render()\n",
        "\n",
        "        return obs, reward, done, False, {}\n",
        "\n",
        "\n",
        "    def _get_obs(self):\n",
        "      grid = np.zeros((self.size, self.size), dtype=np.uint8)\n",
        "\n",
        "      for segment in self.snake:\n",
        "          x, y = segment\n",
        "          if 0 <= x < self.size and 0 <= y < self.size:\n",
        "              grid[y, x] = 1  # snake\n",
        "\n",
        "      if self.food:\n",
        "          fx, fy = self.food\n",
        "          if 0 <= fx < self.size and 0 <= fy < self.size:\n",
        "              grid[fy, fx] = 2  # food\n",
        "\n",
        "      return grid\n",
        "\n",
        "    def _place_food(self):\n",
        "        positions = set(tuple(p) for p in self.snake)\n",
        "        empty = [(x, y) for x in range(self.size) for y in range(self.size) if (x, y) not in positions]\n",
        "        self.food = list(random.choice(empty)) if empty else None\n",
        "\n",
        "    def render(self):\n",
        "        if self.screen is None:\n",
        "            self._render_init()\n",
        "\n",
        "        self.screen.fill((0, 0, 0))\n",
        "        for x, y in self.snake:\n",
        "            pygame.draw.rect(\n",
        "                self.screen, (0, 255, 0),\n",
        "                pygame.Rect(x * self.cell_size, y * self.cell_size, self.cell_size, self.cell_size)\n",
        "            )\n",
        "        if self.food:\n",
        "            fx, fy = self.food\n",
        "            pygame.draw.rect(\n",
        "                self.screen, (255, 0, 0),\n",
        "                pygame.Rect(fx * self.cell_size, fy * self.cell_size, self.cell_size, self.cell_size)\n",
        "            )\n",
        "\n",
        "        pygame.display.flip()\n",
        "        self.clock.tick(self.metadata[\"render_fps\"])\n",
        "\n",
        "    def _render_init(self):\n",
        "        pygame.init()\n",
        "        self.screen = pygame.display.set_mode((self.size * self.cell_size, self.size * self.cell_size))\n",
        "        self.clock = pygame.time.Clock()\n",
        "\n",
        "    def close(self):\n",
        "        if self.screen:\n",
        "            pygame.quit()\n",
        "            self.screen = None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "id": "7d3ffb7a",
      "metadata": {
        "id": "7d3ffb7a"
      },
      "outputs": [],
      "source": [
        "# TODO: Implement training logic for Snake Game here\n",
        "def train_dqn_snake(env,episodes=500,gamma=0.99,epsilon_start=1,epsilon_min=0.1,decay=0.995,learning_rate=0.001,batch_size=64,buffer_size=10000,freq=10):\n",
        "    obs_dim = env.size*env.size\n",
        "    action_dim=env.action_space.n\n",
        "\n",
        "    policy_net=DQN(obs_dim,action_dim)\n",
        "    target_net=DQN(obs_dim,action_dim)\n",
        "\n",
        "    target_net.load_state_dict(policy_net.state_dict())\n",
        "    target_net.eval()\n",
        "\n",
        "    optimizer = optim.Adam(policy_net.parameters(), lr=learning_rate)\n",
        "    buffer = ExperienceBuffer(buffer_size)\n",
        "    target_update = freq\n",
        "    epsilon=epsilon_start\n",
        "\n",
        "    #moving through all the episodes\n",
        "    for episode in range(episodes):\n",
        "      obs,_=env.reset()\n",
        "      total_reward=0\n",
        "      done = False\n",
        "\n",
        "      #while the game is running\n",
        "      while done==False:\n",
        "        state = torch.tensor(obs.flatten(), dtype=torch.float32).unsqueeze(0)\n",
        "        #choose an action via e-greedy algo\n",
        "        if random.random() <epsilon:\n",
        "            action=env.action_space.sample()\n",
        "        else:\n",
        "          with torch.no_grad():\n",
        "              q_values = policy_net(state)\n",
        "          action = torch.argmax(q_values, dim=1).item()\n",
        "\n",
        "\n",
        "\n",
        "        #getting new observation after taking that action\n",
        "        next_obs,reward,done,_,_=env.step(action)\n",
        "        #pushing it into the action replay buffer\n",
        "        buffer.push(obs.flatten(), reward, action, next_obs.flatten(), done)\n",
        "\n",
        "        obs=next_obs\n",
        "        total_reward+=reward\n",
        "\n",
        "        if len(buffer) > batch_size:\n",
        "                states, rewards,actions, next_states, dones = buffer.sample(batch_size)\n",
        "                states = torch.tensor(states, dtype=torch.float32)  # Convert to tensor\n",
        "                actions = torch.tensor(actions, dtype=torch.long)\n",
        "                rewards = torch.tensor(rewards, dtype=torch.float32)\n",
        "                next_states = torch.tensor(next_states, dtype=torch.float32)\n",
        "                dones = torch.tensor(dones, dtype=torch.float32)\n",
        "\n",
        "                q_values = policy_net(states).gather(1, actions.unsqueeze(1)).squeeze()\n",
        "\n",
        "                with torch.no_grad():\n",
        "                    q_next = target_net(next_states).max(1)[0]\n",
        "                    target = rewards + gamma * q_next * (1 - dones)\n",
        "\n",
        "                loss = F.mse_loss(q_values, target)\n",
        "                optimizer.zero_grad()\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "\n",
        "      if episode % target_update == 0:\n",
        "          target_net.load_state_dict(policy_net.state_dict())\n",
        "\n",
        "      epsilon = max(epsilon_min, epsilon * decay)\n",
        "\n",
        "      if (episode + 1) % 10 == 0:\n",
        "          print(f\"Episode {episode + 1}: Total Reward = {total_reward:.2f}, Epsilon = {epsilon:.3f}\")\n",
        "\n",
        "    return policy_net\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "id": "a0e10565",
      "metadata": {
        "id": "a0e10565"
      },
      "outputs": [],
      "source": [
        "def evaluate_snake_model(model, size=20, episodes=10, render=True):\n",
        "    env = SnakeGame(size=size, render_mode=\"human\" if render else None)\n",
        "    model.eval()\n",
        "\n",
        "    rewards = []\n",
        "\n",
        "    for episode in range(episodes):\n",
        "        obs, _ = env.reset()\n",
        "        total_reward = 0\n",
        "        done = False\n",
        "\n",
        "        while not done:\n",
        "            # Flatten the observation before passing to the model\n",
        "            state = torch.tensor(obs.flatten(), dtype=torch.float32).unsqueeze(0)\n",
        "            with torch.no_grad():\n",
        "                q_values = model(state)\n",
        "                action = torch.argmax(q_values, dim=1).item()\n",
        "\n",
        "            obs, reward, done, _, _ = env.step(action)\n",
        "            total_reward += reward\n",
        "\n",
        "            if render:\n",
        "                env.render()\n",
        "\n",
        "        rewards.append(total_reward)\n",
        "        print(f\"Episode {episode + 1}: Reward = {round(total_reward, 2)}\")\n",
        "\n",
        "    env.close()\n",
        "    avg_reward = sum(rewards) / episodes\n",
        "    print(f\"Average reward over {episodes} episodes: {round(avg_reward, 2)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1a7eb581",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1a7eb581",
        "outputId": "ef027109-eb55-414b-8c87-09fa8e02bc2b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode 10: Total Reward = -7.80, Epsilon = 0.951\n",
            "Episode 20: Total Reward = -9.70, Epsilon = 0.905\n",
            "Episode 30: Total Reward = -4.00, Epsilon = 0.860\n",
            "Episode 40: Total Reward = -2.50, Epsilon = 0.818\n",
            "Episode 50: Total Reward = -2.30, Epsilon = 0.778\n",
            "Episode 60: Total Reward = -8.80, Epsilon = 0.740\n",
            "Episode 70: Total Reward = 1.10, Epsilon = 0.704\n",
            "Episode 80: Total Reward = -7.10, Epsilon = 0.670\n",
            "Episode 90: Total Reward = -3.30, Epsilon = 0.637\n",
            "Episode 100: Total Reward = -3.40, Epsilon = 0.606\n",
            "Episode 110: Total Reward = -4.20, Epsilon = 0.576\n",
            "Episode 120: Total Reward = -7.50, Epsilon = 0.548\n",
            "Episode 130: Total Reward = -4.90, Epsilon = 0.521\n",
            "Episode 140: Total Reward = -4.90, Epsilon = 0.496\n",
            "Episode 150: Total Reward = -3.40, Epsilon = 0.471\n",
            "Episode 160: Total Reward = -6.30, Epsilon = 0.448\n",
            "Episode 170: Total Reward = -3.50, Epsilon = 0.427\n",
            "Episode 180: Total Reward = -4.40, Epsilon = 0.406\n",
            "Episode 190: Total Reward = -4.60, Epsilon = 0.386\n",
            "Episode 200: Total Reward = 6.80, Epsilon = 0.367\n",
            "Episode 210: Total Reward = -1.90, Epsilon = 0.349\n",
            "Episode 220: Total Reward = -5.70, Epsilon = 0.332\n",
            "Episode 230: Total Reward = -8.00, Epsilon = 0.316\n"
          ]
        }
      ],
      "source": [
        "\n",
        "env = SnakeGame(size=20, render_mode=None)  # Or adjust the size as needed\n",
        "trained_model = train_dqn_snake(env)\n",
        "evaluate_snake_model(trained_model, size=20, episodes=10, render=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4224fabe",
      "metadata": {
        "id": "4224fabe"
      },
      "outputs": [],
      "source": [
        "class ChaseEscapeEnv(gym.Env):\n",
        "    metadata = {\"render_modes\": [\"human\"], \"render_fps\": 30}\n",
        "\n",
        "    def __init__(self, render_mode=None):\n",
        "        super().__init__()\n",
        "\n",
        "        self.dt = 0.1\n",
        "        self.max_speed = 0.4\n",
        "        self.agent_radius = 0.05\n",
        "        self.target_radius = 0.05\n",
        "        self.chaser_radius = 0.07\n",
        "        self.chaser_speed = 0.03\n",
        "\n",
        "        self.action_space = gym.spaces.MultiDiscrete([3, 3])  # actions in {0,1,2} map to [-1,0,1]\n",
        "        self.observation_space = gym.spaces.Box(\n",
        "            low=-1,\n",
        "            high=1,\n",
        "            shape=(8,),\n",
        "            dtype=np.float32,\n",
        "        )\n",
        "\n",
        "        self.render_mode = render_mode\n",
        "        self.screen_size = 500\n",
        "        self.np_random = None\n",
        "\n",
        "        if render_mode == \"human\":\n",
        "            pygame.init()\n",
        "            self.screen = pygame.display.set_mode((self.screen_size, self.screen_size))\n",
        "            self.clock = pygame.time.Clock()\n",
        "\n",
        "    def sample_pos(self, far_from=None, min_dist=0.5):\n",
        "        while True:\n",
        "            pos = self.np_random.uniform(low=-0.8, high=0.8, size=(2,))\n",
        "            if far_from is None or np.linalg.norm(pos - far_from) >= min_dist:\n",
        "                return pos\n",
        "\n",
        "    def reset(self, seed=None, options=None):\n",
        "        super().reset(seed=seed)\n",
        "\n",
        "        self.agent_pos = self.sample_pos()\n",
        "        self.agent_vel = np.zeros(2, dtype=np.float32)\n",
        "        self.target_pos = self.sample_pos(far_from=self.agent_pos, min_dist=0.5)\n",
        "        self.chaser_pos = self.sample_pos(far_from=self.agent_pos, min_dist=0.7)\n",
        "\n",
        "        return self._get_obs(), {}\n",
        "\n",
        "    def _get_obs(self):\n",
        "        # TODO: Decide how to pass the state (don't use pixel values)\n",
        "        pass\n",
        "\n",
        "    def _get_info(self):\n",
        "        return {}\n",
        "\n",
        "    def step(self, action):\n",
        "        # TODO: Add reward scheme\n",
        "        # 1) Try to make the agent stay within bounds\n",
        "        # 2) The agent shouldn't idle around\n",
        "        # 3) The agent should go for the reward\n",
        "        # 4) The agent should avoid the chaser\n",
        "\n",
        "        accel = (np.array(action) - 1) * 0.1\n",
        "        self.agent_vel += accel\n",
        "        self.agent_vel = np.clip(self.agent_vel, -self.max_speed, self.max_speed)\n",
        "        self.agent_pos += self.agent_vel * self.dt\n",
        "        self.agent_pos = np.clip(self.agent_pos, -1, 1)\n",
        "\n",
        "        direction = self.agent_pos - self.chaser_pos\n",
        "        norm = np.linalg.norm(direction)\n",
        "        if norm > 1e-5:\n",
        "            self.chaser_pos += self.chaser_speed * direction / norm\n",
        "\n",
        "        dist_to_target = np.linalg.norm(self.agent_pos - self.target_pos)\n",
        "        dist_to_chaser = np.linalg.norm(self.agent_pos - self.chaser_pos)\n",
        "\n",
        "        reward = 0.0\n",
        "        terminated = False\n",
        "\n",
        "        if dist_to_target < self.agent_radius + self.target_radius:\n",
        "            self.target_pos = self.sample_pos(far_from=self.agent_pos, min_dist=0.5)\n",
        "\n",
        "        if dist_to_chaser < self.agent_radius + self.chaser_radius:\n",
        "            terminated = True\n",
        "\n",
        "        return self._get_obs(), reward, terminated, False, self._get_info()\n",
        "\n",
        "    def render(self):\n",
        "        if self.render_mode != \"human\":\n",
        "            return\n",
        "\n",
        "        for event in pygame.event.get():\n",
        "            if event.type == pygame.QUIT:\n",
        "                self.close()\n",
        "\n",
        "        self.screen.fill((255, 255, 255))\n",
        "\n",
        "        def to_screen(p):\n",
        "            x = int((p[0] + 1) / 2 * self.screen_size)\n",
        "            y = int((1 - (p[1] + 1) / 2) * self.screen_size)\n",
        "            return x, y\n",
        "\n",
        "        pygame.draw.circle(self.screen, (0, 255, 0), to_screen(self.target_pos), int(self.target_radius * self.screen_size))\n",
        "        pygame.draw.circle(self.screen, (0, 0, 255), to_screen(self.agent_pos), int(self.agent_radius * self.screen_size))\n",
        "        pygame.draw.circle(self.screen, (255, 0, 0), to_screen(self.chaser_pos), int(self.chaser_radius * self.screen_size))\n",
        "\n",
        "        pygame.display.flip()\n",
        "        self.clock.tick(self.metadata[\"render_fps\"])\n",
        "\n",
        "    def close(self):\n",
        "        if self.render_mode == \"human\":\n",
        "            pygame.quit()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9eca1390",
      "metadata": {
        "id": "9eca1390"
      },
      "outputs": [],
      "source": [
        "# TODO: Train and evaluate CatMouseEnv"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "l4JXK5WPaksw"
      },
      "id": "l4JXK5WPaksw",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.3"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}