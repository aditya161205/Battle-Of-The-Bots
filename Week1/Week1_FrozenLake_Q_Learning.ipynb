{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "4a44e85e",
      "metadata": {
        "id": "4a44e85e"
      },
      "source": [
        "# Week 1: Q-Learning on FrozenLake"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "857c76b5",
      "metadata": {
        "id": "857c76b5"
      },
      "source": [
        "Welcome to Week 1! This week we’ll get hands-on with **Q-learning** using OpenAI Gym’s **FrozenLake-v1** environment. Your goal is to train a tabular Q-learning agent to navigate a slippery frozen lake without falling into holes."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dd99add7",
      "metadata": {
        "id": "dd99add7"
      },
      "source": [
        "Read about the FrozenLake environment from the openAI gymnasium website and try to solve this assignment"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "27df57cf",
      "metadata": {
        "id": "27df57cf"
      },
      "source": [
        "## Goals"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7640e346",
      "metadata": {
        "id": "7640e346"
      },
      "source": [
        "- Understand and implement Q-learning with a Q-table.\n",
        "- Use an ε-greedy exploration strategy.\n",
        "- Visualize training progress with reward curves.\n",
        "- Evaluate the learned policy.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "095f18d9",
      "metadata": {
        "id": "095f18d9"
      },
      "source": [
        "## Environment Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "b83ea1c0",
      "metadata": {
        "id": "b83ea1c0",
        "outputId": "9e9c25ae-3dde-423c-a592-2425a850055d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gym==0.26.2 in /usr/local/lib/python3.11/dist-packages (0.26.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (3.10.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (2.0.2)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from gym==0.26.2) (3.1.1)\n",
            "Requirement already satisfied: gym_notices>=0.0.4 in /usr/local/lib/python3.11/dist-packages (from gym==0.26.2) (0.0.8)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (4.58.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.4.8)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (24.2)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (11.2.1)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (2.9.0.post0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install gym==0.26.2 matplotlib numpy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "939863ed",
      "metadata": {
        "id": "939863ed"
      },
      "outputs": [],
      "source": [
        "import gym\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "\n",
        "env = gym.make(\"FrozenLake-v1\", is_slippery=False)\n",
        "state_space_size = env.observation_space.n\n",
        "action_space_size = env.action_space.n\n",
        "Q = np.zeros((state_space_size, action_space_size))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "46fc9db4",
      "metadata": {
        "id": "46fc9db4"
      },
      "source": [
        "## Training Loop\n",
        "Complete the training loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "43c44821",
      "metadata": {
        "id": "43c44821"
      },
      "outputs": [],
      "source": [
        "def train_agent(episodes=2000, alpha=0.8, gamma=0.95, epsilon=1.0, decay=0.995):\n",
        "    rewards = []\n",
        "    for ep in range(episodes):\n",
        "        state = env.reset()[0]\n",
        "        done = False\n",
        "        total_reward = 0\n",
        "        while not done:\n",
        "            pass\n",
        "            if random.random() < epsilon:\n",
        "                action = random.choice(range(action_space_size))\n",
        "            else:\n",
        "                action = np.argmax(Q[state])\n",
        "\n",
        "            next_state, reward, done, _, _ = env.step(action)\n",
        "\n",
        "            Q[state, action] = Q[state, action] + alpha * (reward + gamma * max(Q[next_state]) - Q[state, action])\n",
        "\n",
        "\n",
        "            state = next_state\n",
        "            total_reward += reward\n",
        "\n",
        "        epsilon *= decay\n",
        "\n",
        "        rewards.append(total_reward)\n",
        "    return rewards"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3a650167",
      "metadata": {
        "id": "3a650167"
      },
      "source": [
        "## Plotting Results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8dbf9934",
      "metadata": {
        "id": "8dbf9934"
      },
      "outputs": [],
      "source": [
        "rewards = train_agent()\n",
        "plt.plot(np.convolve(rewards, np.ones(100)/100, mode='valid'))\n",
        "plt.title(\"100-Episode Moving Average of Rewards\")\n",
        "plt.xlabel(\"Episode\")\n",
        "plt.ylabel(\"Average Reward\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "73350b54",
      "metadata": {
        "id": "73350b54"
      },
      "source": [
        "## Test the Learned Policy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "96c85213",
      "metadata": {
        "id": "96c85213"
      },
      "outputs": [],
      "source": [
        "def test_agent(Q, episodes=5):\n",
        "    for ep in range(episodes):\n",
        "        state = env.reset()[0]\n",
        "        done = False\n",
        "        print(f\"Episode {ep + 1}\")\n",
        "        while not done:\n",
        "            action = np.argmax(Q[state])\n",
        "            next_state, reward, done, _, _ = env.step(action)\n",
        "            env.render()\n",
        "            state = next_state"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "385e9a4c",
      "metadata": {
        "id": "385e9a4c",
        "outputId": "7d1bdae0-c774-4035-9c29-097f6452e621",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode 1\n",
            "Episode 2\n",
            "Episode 3\n",
            "Episode 4\n",
            "Episode 5\n"
          ]
        }
      ],
      "source": [
        "test_agent(Q)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9da86d73",
      "metadata": {
        "id": "9da86d73"
      },
      "source": [
        "## Challenges"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f0d2f674",
      "metadata": {
        "id": "f0d2f674"
      },
      "source": [
        "1. Set `is_slippery=False` and compare performance.\n",
        "2. Change the reward for falling into holes.\n",
        "3. Add a decaying learning rate `α = α0 / (1 + decay * t)`.\n",
        "4. Visualize the Q-table as a heatmap (optional).\n",
        "5. Maybe try to think about how to generalize this to solve a random lake without pretraining on the specific environment(Post your ideas on the whatsapp group and we will host a competition if people are interested)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "77fa77a8",
      "metadata": {
        "id": "77fa77a8"
      },
      "source": [
        "## TLDR"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b7b02a3e",
      "metadata": {
        "id": "b7b02a3e"
      },
      "source": [
        "Learn how to implement tabular Q-learning to solve a simple environment. Use exploration, value updates, and reward tracking to build intuition before moving to deep RL.\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.13.3"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}